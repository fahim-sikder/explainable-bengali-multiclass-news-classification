{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "504686e1-8bfd-486a-a921-536ed8cde10d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fahim/anaconda3/envs/explainable-news/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-03-10 17:49:46.122795: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-10 17:49:46.538209: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.7/lib64:/usr/local/cuda-11.7/lib64\n",
      "2023-03-10 17:49:46.538260: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.7/lib64:/usr/local/cuda-11.7/lib64\n",
      "2023-03-10 17:49:46.538264: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# Some reference is taken from here: https://captum.ai/tutorials/Bert_SQUAD_Interpret\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from captum.attr import LayerIntegratedGradients\n",
    "from captum.attr import visualization as viz\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pickle\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38b98beb-3fb3-424a-828f-0625a62db386",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fahim/anaconda3/envs/explainable-news/lib/python3.8/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LabelEncoder from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "label_enc = LabelEncoder()\n",
    "\n",
    "with open(f'label_encoder.pkl', 'rb')as f:\n",
    "    \n",
    "    label_enc = pickle.load(f)\n",
    "    \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2badaa8-ca74-4c4e-93c8-9c294f6c5e40",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['bangladesh', 'economy', 'education', 'entertainment',\n",
       "       'international', 'life-style', 'opinion', 'sports', 'technology'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_enc.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26917f91-5e72-45a5-9fbf-7172cc1c2384",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type electra to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at csebuetnlp/banglabert_large were not used when initializing BertModel: ['electra.encoder.layer.7.attention.self.value.bias', 'electra.encoder.layer.19.output.dense.weight', 'electra.encoder.layer.14.intermediate.dense.weight', 'electra.encoder.layer.13.attention.self.key.weight', 'electra.encoder.layer.7.intermediate.dense.bias', 'electra.encoder.layer.12.attention.self.value.weight', 'electra.encoder.layer.22.attention.self.query.weight', 'electra.encoder.layer.11.intermediate.dense.weight', 'electra.encoder.layer.9.intermediate.dense.weight', 'electra.encoder.layer.5.attention.self.query.weight', 'electra.encoder.layer.19.attention.self.key.bias', 'electra.encoder.layer.11.output.LayerNorm.weight', 'electra.encoder.layer.19.output.LayerNorm.bias', 'electra.encoder.layer.22.output.LayerNorm.bias', 'electra.encoder.layer.1.attention.output.dense.bias', 'electra.encoder.layer.14.attention.self.value.weight', 'electra.encoder.layer.20.attention.self.value.bias', 'electra.encoder.layer.16.output.LayerNorm.bias', 'electra.encoder.layer.23.attention.output.LayerNorm.bias', 'electra.encoder.layer.16.attention.self.query.bias', 'electra.encoder.layer.10.attention.output.dense.weight', 'electra.encoder.layer.2.attention.output.dense.weight', 'electra.encoder.layer.21.intermediate.dense.bias', 'electra.encoder.layer.10.attention.output.dense.bias', 'electra.encoder.layer.2.output.LayerNorm.weight', 'electra.encoder.layer.16.attention.output.LayerNorm.bias', 'electra.encoder.layer.15.output.dense.weight', 'electra.encoder.layer.3.attention.self.key.bias', 'electra.encoder.layer.5.intermediate.dense.bias', 'electra.encoder.layer.4.intermediate.dense.weight', 'electra.encoder.layer.10.attention.self.key.weight', 'electra.encoder.layer.19.attention.output.dense.bias', 'electra.encoder.layer.1.attention.self.value.bias', 'electra.encoder.layer.18.output.LayerNorm.weight', 'electra.encoder.layer.14.output.dense.bias', 'electra.encoder.layer.14.attention.self.value.bias', 'electra.encoder.layer.4.attention.output.LayerNorm.weight', 'electra.encoder.layer.17.attention.self.key.weight', 'electra.encoder.layer.5.attention.output.dense.bias', 'electra.encoder.layer.2.attention.self.value.weight', 'electra.encoder.layer.5.output.LayerNorm.weight', 'electra.encoder.layer.8.attention.self.query.weight', 'electra.encoder.layer.15.attention.output.LayerNorm.weight', 'electra.encoder.layer.9.attention.output.LayerNorm.weight', 'electra.encoder.layer.23.intermediate.dense.weight', 'electra.encoder.layer.18.attention.self.key.bias', 'electra.encoder.layer.5.attention.self.query.bias', 'electra.encoder.layer.8.attention.self.value.bias', 'electra.encoder.layer.8.attention.output.dense.bias', 'electra.encoder.layer.23.attention.self.key.bias', 'electra.encoder.layer.20.output.LayerNorm.weight', 'electra.encoder.layer.14.attention.self.query.bias', 'electra.encoder.layer.0.attention.output.dense.weight', 'electra.encoder.layer.21.attention.self.query.weight', 'electra.encoder.layer.6.intermediate.dense.weight', 'electra.encoder.layer.1.attention.output.dense.weight', 'electra.encoder.layer.20.output.LayerNorm.bias', 'electra.encoder.layer.22.attention.output.LayerNorm.bias', 'electra.encoder.layer.0.attention.self.value.weight', 'electra.encoder.layer.13.attention.output.dense.bias', 'electra.encoder.layer.18.attention.self.query.bias', 'electra.encoder.layer.12.attention.self.query.weight', 'electra.encoder.layer.23.attention.output.dense.bias', 'electra.encoder.layer.19.attention.self.query.weight', 'electra.encoder.layer.19.attention.self.query.bias', 'electra.encoder.layer.7.intermediate.dense.weight', 'electra.encoder.layer.8.attention.self.key.bias', 'electra.encoder.layer.21.attention.self.query.bias', 'electra.encoder.layer.0.attention.self.query.bias', 'electra.encoder.layer.2.output.LayerNorm.bias', 'electra.encoder.layer.21.attention.self.key.bias', 'electra.encoder.layer.3.output.dense.bias', 'electra.encoder.layer.5.attention.self.key.bias', 'electra.encoder.layer.19.attention.self.value.weight', 'electra.encoder.layer.11.output.LayerNorm.bias', 'electra.encoder.layer.22.attention.self.key.bias', 'discriminator_predictions.dense_prediction.weight', 'electra.encoder.layer.2.intermediate.dense.bias', 'electra.encoder.layer.5.intermediate.dense.weight', 'electra.encoder.layer.16.output.dense.weight', 'electra.encoder.layer.13.attention.self.query.bias', 'electra.encoder.layer.13.output.LayerNorm.bias', 'electra.encoder.layer.12.attention.output.LayerNorm.bias', 'electra.encoder.layer.13.output.dense.weight', 'electra.encoder.layer.4.output.LayerNorm.weight', 'electra.encoder.layer.5.output.LayerNorm.bias', 'electra.encoder.layer.19.attention.output.LayerNorm.bias', 'electra.encoder.layer.15.attention.self.key.weight', 'electra.encoder.layer.22.attention.output.LayerNorm.weight', 'electra.encoder.layer.1.intermediate.dense.weight', 'electra.encoder.layer.10.attention.output.LayerNorm.weight', 'electra.encoder.layer.22.attention.self.value.weight', 'electra.encoder.layer.7.attention.self.key.bias', 'electra.encoder.layer.11.attention.output.LayerNorm.weight', 'electra.encoder.layer.13.intermediate.dense.weight', 'electra.encoder.layer.23.output.dense.bias', 'electra.encoder.layer.2.attention.self.key.weight', 'electra.embeddings.word_embeddings.weight', 'electra.encoder.layer.10.attention.output.LayerNorm.bias', 'electra.encoder.layer.14.output.LayerNorm.weight', 'electra.encoder.layer.20.attention.output.LayerNorm.bias', 'electra.encoder.layer.6.attention.self.key.weight', 'electra.encoder.layer.7.output.LayerNorm.bias', 'electra.encoder.layer.20.attention.output.dense.weight', 'electra.encoder.layer.7.attention.self.query.weight', 'electra.encoder.layer.20.output.dense.bias', 'electra.encoder.layer.11.attention.output.dense.bias', 'electra.encoder.layer.20.intermediate.dense.bias', 'electra.encoder.layer.12.output.dense.bias', 'electra.encoder.layer.2.attention.self.value.bias', 'electra.encoder.layer.9.attention.self.query.weight', 'electra.encoder.layer.12.output.LayerNorm.bias', 'electra.encoder.layer.3.intermediate.dense.weight', 'electra.encoder.layer.10.attention.self.query.weight', 'electra.encoder.layer.11.attention.output.dense.weight', 'electra.encoder.layer.17.output.LayerNorm.bias', 'electra.encoder.layer.18.attention.self.value.weight', 'electra.encoder.layer.9.output.LayerNorm.bias', 'electra.encoder.layer.3.output.LayerNorm.weight', 'electra.encoder.layer.17.attention.output.LayerNorm.bias', 'electra.encoder.layer.17.intermediate.dense.bias', 'electra.encoder.layer.13.attention.output.LayerNorm.weight', 'electra.encoder.layer.8.intermediate.dense.weight', 'electra.encoder.layer.11.output.dense.weight', 'electra.encoder.layer.3.attention.self.value.weight', 'electra.encoder.layer.16.output.dense.bias', 'electra.encoder.layer.6.attention.self.key.bias', 'electra.encoder.layer.16.attention.output.dense.bias', 'electra.encoder.layer.13.attention.output.dense.weight', 'electra.encoder.layer.10.attention.self.value.weight', 'electra.encoder.layer.1.attention.self.key.weight', 'electra.encoder.layer.23.output.LayerNorm.bias', 'electra.encoder.layer.6.attention.output.dense.bias', 'electra.encoder.layer.3.attention.self.key.weight', 'electra.encoder.layer.20.attention.self.value.weight', 'electra.encoder.layer.3.attention.output.LayerNorm.bias', 'electra.encoder.layer.19.intermediate.dense.weight', 'electra.encoder.layer.21.attention.self.key.weight', 'electra.encoder.layer.2.intermediate.dense.weight', 'electra.encoder.layer.23.attention.self.key.weight', 'electra.encoder.layer.11.attention.self.key.weight', 'electra.encoder.layer.22.intermediate.dense.weight', 'electra.encoder.layer.10.output.dense.weight', 'electra.encoder.layer.1.output.dense.weight', 'electra.encoder.layer.23.output.LayerNorm.weight', 'discriminator_predictions.dense_prediction.bias', 'electra.encoder.layer.16.attention.self.value.weight', 'electra.encoder.layer.21.intermediate.dense.weight', 'electra.encoder.layer.20.attention.self.key.weight', 'electra.encoder.layer.2.attention.output.LayerNorm.weight', 'electra.encoder.layer.19.attention.self.value.bias', 'electra.encoder.layer.0.attention.output.dense.bias', 'electra.encoder.layer.0.output.dense.bias', 'electra.encoder.layer.15.output.LayerNorm.bias', 'electra.embeddings.LayerNorm.bias', 'electra.encoder.layer.6.attention.self.value.weight', 'electra.encoder.layer.21.output.dense.weight', 'electra.encoder.layer.2.output.dense.bias', 'electra.encoder.layer.22.intermediate.dense.bias', 'electra.encoder.layer.7.output.LayerNorm.weight', 'electra.encoder.layer.14.attention.output.dense.weight', 'electra.encoder.layer.17.attention.self.query.bias', 'electra.encoder.layer.6.output.LayerNorm.bias', 'electra.encoder.layer.3.attention.self.value.bias', 'electra.encoder.layer.20.attention.output.dense.bias', 'electra.encoder.layer.7.attention.output.dense.bias', 'electra.encoder.layer.23.attention.self.query.bias', 'electra.encoder.layer.21.attention.self.value.bias', 'electra.encoder.layer.15.intermediate.dense.weight', 'electra.encoder.layer.3.output.dense.weight', 'electra.encoder.layer.22.output.dense.bias', 'electra.encoder.layer.15.attention.self.value.weight', 'electra.encoder.layer.11.attention.self.query.bias', 'electra.encoder.layer.14.output.dense.weight', 'electra.encoder.layer.1.attention.self.query.weight', 'electra.encoder.layer.18.output.dense.bias', 'electra.encoder.layer.13.attention.self.value.bias', 'electra.encoder.layer.4.attention.output.LayerNorm.bias', 'electra.encoder.layer.20.attention.self.query.bias', 'electra.encoder.layer.7.output.dense.bias', 'electra.encoder.layer.9.attention.output.dense.weight', 'electra.encoder.layer.17.intermediate.dense.weight', 'electra.encoder.layer.20.intermediate.dense.weight', 'electra.encoder.layer.16.intermediate.dense.weight', 'electra.encoder.layer.12.intermediate.dense.weight', 'electra.encoder.layer.4.attention.self.query.weight', 'electra.encoder.layer.16.attention.output.LayerNorm.weight', 'electra.encoder.layer.16.attention.output.dense.weight', 'electra.encoder.layer.4.attention.self.query.bias', 'electra.encoder.layer.8.attention.output.LayerNorm.weight', 'electra.encoder.layer.17.attention.self.value.weight', 'electra.encoder.layer.23.attention.self.value.bias', 'electra.encoder.layer.9.attention.self.value.weight', 'electra.encoder.layer.23.output.dense.weight', 'electra.encoder.layer.9.attention.self.key.bias', 'electra.encoder.layer.11.output.dense.bias', 'electra.encoder.layer.7.attention.output.LayerNorm.bias', 'electra.encoder.layer.18.attention.output.dense.bias', 'electra.encoder.layer.18.output.LayerNorm.bias', 'electra.encoder.layer.15.attention.output.dense.bias', 'electra.encoder.layer.12.output.LayerNorm.weight', 'electra.encoder.layer.0.output.LayerNorm.bias', 'electra.encoder.layer.5.attention.output.dense.weight', 'electra.encoder.layer.16.attention.self.key.weight', 'electra.encoder.layer.4.attention.self.key.bias', 'electra.encoder.layer.17.output.LayerNorm.weight', 'electra.encoder.layer.1.attention.output.LayerNorm.bias', 'electra.encoder.layer.9.attention.self.query.bias', 'electra.encoder.layer.2.attention.output.dense.bias', 'electra.encoder.layer.11.attention.self.value.weight', 'electra.encoder.layer.4.attention.self.value.weight', 'electra.encoder.layer.22.output.dense.weight', 'electra.encoder.layer.21.attention.output.LayerNorm.bias', 'electra.embeddings.token_type_embeddings.weight', 'electra.encoder.layer.3.attention.output.LayerNorm.weight', 'electra.encoder.layer.2.attention.self.query.weight', 'electra.encoder.layer.21.attention.output.LayerNorm.weight', 'electra.encoder.layer.2.output.dense.weight', 'electra.encoder.layer.18.attention.output.dense.weight', 'electra.encoder.layer.4.intermediate.dense.bias', 'electra.encoder.layer.11.attention.self.key.bias', 'electra.encoder.layer.8.intermediate.dense.bias', 'electra.encoder.layer.1.output.dense.bias', 'electra.encoder.layer.15.attention.output.LayerNorm.bias', 'electra.encoder.layer.0.intermediate.dense.bias', 'electra.encoder.layer.9.attention.output.dense.bias', 'electra.encoder.layer.22.output.LayerNorm.weight', 'electra.encoder.layer.5.attention.self.key.weight', 'electra.encoder.layer.14.attention.output.LayerNorm.bias', 'electra.encoder.layer.6.attention.self.query.bias', 'electra.encoder.layer.6.attention.output.LayerNorm.weight', 'electra.encoder.layer.3.attention.output.dense.weight', 'electra.encoder.layer.8.output.LayerNorm.weight', 'electra.encoder.layer.18.attention.self.value.bias', 'electra.encoder.layer.0.attention.self.value.bias', 'electra.encoder.layer.6.attention.output.dense.weight', 'electra.encoder.layer.10.output.dense.bias', 'electra.encoder.layer.8.output.dense.bias', 'electra.encoder.layer.23.attention.self.value.weight', 'electra.encoder.layer.14.attention.self.query.weight', 'electra.encoder.layer.23.attention.output.dense.weight', 'electra.encoder.layer.17.attention.output.LayerNorm.weight', 'electra.encoder.layer.17.attention.self.value.bias', 'electra.encoder.layer.20.attention.output.LayerNorm.weight', 'electra.encoder.layer.17.attention.output.dense.bias', 'electra.encoder.layer.14.attention.self.key.weight', 'electra.encoder.layer.15.output.dense.bias', 'electra.encoder.layer.13.attention.self.key.bias', 'electra.encoder.layer.5.attention.output.LayerNorm.bias', 'electra.encoder.layer.3.intermediate.dense.bias', 'electra.encoder.layer.22.attention.self.query.bias', 'electra.encoder.layer.8.output.dense.weight', 'electra.encoder.layer.0.attention.output.LayerNorm.weight', 'electra.encoder.layer.13.output.LayerNorm.weight', 'electra.encoder.layer.9.attention.self.value.bias', 'electra.encoder.layer.8.attention.output.LayerNorm.bias', 'electra.encoder.layer.23.attention.self.query.weight', 'electra.encoder.layer.6.output.dense.bias', 'electra.encoder.layer.3.output.LayerNorm.bias', 'electra.encoder.layer.8.attention.self.query.bias', 'electra.encoder.layer.6.intermediate.dense.bias', 'electra.encoder.layer.4.attention.output.dense.bias', 'electra.encoder.layer.11.attention.self.value.bias', 'electra.encoder.layer.12.attention.self.key.bias', 'electra.embeddings.LayerNorm.weight', 'electra.encoder.layer.0.attention.self.key.bias', 'electra.encoder.layer.10.intermediate.dense.bias', 'electra.encoder.layer.10.intermediate.dense.weight', 'electra.encoder.layer.8.attention.self.key.weight', 'electra.encoder.layer.14.attention.self.key.bias', 'electra.encoder.layer.15.intermediate.dense.bias', 'electra.encoder.layer.6.output.LayerNorm.weight', 'electra.encoder.layer.15.attention.self.value.bias', 'electra.encoder.layer.10.attention.self.key.bias', 'electra.encoder.layer.0.attention.self.query.weight', 'electra.encoder.layer.7.attention.output.LayerNorm.weight', 'electra.encoder.layer.7.attention.output.dense.weight', 'electra.encoder.layer.22.attention.self.key.weight', 'discriminator_predictions.dense.bias', 'electra.encoder.layer.21.attention.output.dense.bias', 'electra.encoder.layer.22.attention.output.dense.bias', 'electra.encoder.layer.2.attention.self.query.bias', 'electra.encoder.layer.17.attention.output.dense.weight', 'electra.encoder.layer.18.attention.output.LayerNorm.weight', 'electra.encoder.layer.12.output.dense.weight', 'electra.encoder.layer.6.attention.self.query.weight', 'electra.encoder.layer.12.attention.self.key.weight', 'electra.encoder.layer.13.intermediate.dense.bias', 'electra.encoder.layer.4.output.LayerNorm.bias', 'electra.encoder.layer.5.output.dense.weight', 'electra.encoder.layer.5.attention.output.LayerNorm.weight', 'electra.encoder.layer.15.attention.self.query.bias', 'electra.encoder.layer.1.intermediate.dense.bias', 'electra.encoder.layer.11.attention.output.LayerNorm.bias', 'electra.encoder.layer.2.attention.self.key.bias', 'electra.encoder.layer.15.attention.output.dense.weight', 'electra.encoder.layer.17.attention.self.query.weight', 'electra.encoder.layer.18.intermediate.dense.bias', 'electra.encoder.layer.9.output.dense.bias', 'electra.encoder.layer.10.output.LayerNorm.bias', 'electra.encoder.layer.2.attention.output.LayerNorm.bias', 'electra.encoder.layer.19.attention.output.dense.weight', 'electra.encoder.layer.16.output.LayerNorm.weight', 'electra.encoder.layer.9.attention.self.key.weight', 'electra.encoder.layer.21.output.dense.bias', 'electra.encoder.layer.3.attention.output.dense.bias', 'discriminator_predictions.dense.weight', 'electra.encoder.layer.9.output.dense.weight', 'electra.encoder.layer.1.attention.self.key.bias', 'electra.encoder.layer.14.output.LayerNorm.bias', 'electra.encoder.layer.16.attention.self.key.bias', 'electra.encoder.layer.14.intermediate.dense.bias', 'electra.encoder.layer.1.attention.output.LayerNorm.weight', 'electra.encoder.layer.5.output.dense.bias', 'electra.encoder.layer.0.output.dense.weight', 'electra.encoder.layer.14.attention.output.dense.bias', 'electra.encoder.layer.22.attention.output.dense.weight', 'electra.encoder.layer.6.attention.output.LayerNorm.bias', 'electra.encoder.layer.17.output.dense.weight', 'electra.encoder.layer.11.attention.self.query.weight', 'electra.encoder.layer.18.attention.self.query.weight', 'electra.encoder.layer.22.attention.self.value.bias', 'electra.encoder.layer.18.intermediate.dense.weight', 'electra.encoder.layer.8.output.LayerNorm.bias', 'electra.encoder.layer.8.attention.output.dense.weight', 'electra.encoder.layer.6.attention.self.value.bias', 'electra.encoder.layer.15.attention.self.key.bias', 'electra.encoder.layer.21.attention.self.value.weight', 'electra.embeddings.position_ids', 'electra.encoder.layer.12.attention.output.LayerNorm.weight', 'electra.encoder.layer.7.output.dense.weight', 'electra.encoder.layer.16.attention.self.value.bias', 'electra.encoder.layer.20.attention.self.query.weight', 'electra.encoder.layer.20.attention.self.key.bias', 'electra.encoder.layer.4.attention.self.key.weight', 'electra.encoder.layer.10.attention.self.query.bias', 'electra.encoder.layer.10.output.LayerNorm.weight', 'electra.encoder.layer.0.intermediate.dense.weight', 'electra.encoder.layer.19.attention.output.LayerNorm.weight', 'electra.encoder.layer.7.attention.self.value.weight', 'electra.encoder.layer.20.output.dense.weight', 'electra.encoder.layer.13.attention.output.LayerNorm.bias', 'electra.encoder.layer.21.output.LayerNorm.weight', 'electra.encoder.layer.1.attention.self.value.weight', 'electra.encoder.layer.1.output.LayerNorm.weight', 'electra.encoder.layer.3.attention.self.query.weight', 'electra.encoder.layer.7.attention.self.key.weight', 'electra.encoder.layer.17.attention.self.key.bias', 'electra.encoder.layer.9.output.LayerNorm.weight', 'electra.encoder.layer.18.attention.output.LayerNorm.bias', 'electra.encoder.layer.19.attention.self.key.weight', 'electra.encoder.layer.17.output.dense.bias', 'electra.encoder.layer.13.output.dense.bias', 'electra.encoder.layer.8.attention.self.value.weight', 'electra.encoder.layer.5.attention.self.value.weight', 'electra.encoder.layer.19.output.dense.bias', 'electra.encoder.layer.12.attention.self.value.bias', 'electra.encoder.layer.1.attention.self.query.bias', 'electra.encoder.layer.12.attention.output.dense.bias', 'electra.encoder.layer.15.output.LayerNorm.weight', 'electra.encoder.layer.5.attention.self.value.bias', 'electra.encoder.layer.10.attention.self.value.bias', 'electra.encoder.layer.11.intermediate.dense.bias', 'electra.encoder.layer.3.attention.self.query.bias', 'electra.encoder.layer.12.attention.output.dense.weight', 'electra.encoder.layer.18.output.dense.weight', 'electra.encoder.layer.1.output.LayerNorm.bias', 'electra.encoder.layer.12.intermediate.dense.bias', 'electra.encoder.layer.4.output.dense.bias', 'electra.encoder.layer.6.output.dense.weight', 'electra.embeddings.position_embeddings.weight', 'electra.encoder.layer.13.attention.self.query.weight', 'electra.encoder.layer.7.attention.self.query.bias', 'electra.encoder.layer.4.output.dense.weight', 'electra.encoder.layer.4.attention.output.dense.weight', 'electra.encoder.layer.4.attention.self.value.bias', 'electra.encoder.layer.0.attention.output.LayerNorm.bias', 'electra.encoder.layer.18.attention.self.key.weight', 'electra.encoder.layer.9.intermediate.dense.bias', 'electra.encoder.layer.15.attention.self.query.weight', 'electra.encoder.layer.23.intermediate.dense.bias', 'electra.encoder.layer.23.attention.output.LayerNorm.weight', 'electra.encoder.layer.14.attention.output.LayerNorm.weight', 'electra.encoder.layer.21.attention.output.dense.weight', 'electra.encoder.layer.16.attention.self.query.weight', 'electra.encoder.layer.13.attention.self.value.weight', 'electra.encoder.layer.9.attention.output.LayerNorm.bias', 'electra.encoder.layer.0.attention.self.key.weight', 'electra.encoder.layer.19.intermediate.dense.bias', 'electra.encoder.layer.19.output.LayerNorm.weight', 'electra.encoder.layer.12.attention.self.query.bias', 'electra.encoder.layer.21.output.LayerNorm.bias', 'electra.encoder.layer.0.output.LayerNorm.weight', 'electra.encoder.layer.16.intermediate.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at csebuetnlp/banglabert_large and are newly initialized: ['encoder.layer.0.intermediate.dense.bias', 'encoder.layer.22.attention.self.value.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.12.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.14.intermediate.dense.weight', 'encoder.layer.19.attention.output.LayerNorm.bias', 'encoder.layer.13.output.dense.weight', 'encoder.layer.22.attention.self.query.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.19.attention.self.key.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.20.attention.output.LayerNorm.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.19.attention.self.value.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.22.intermediate.dense.weight', 'encoder.layer.13.attention.self.value.bias', 'encoder.layer.14.attention.output.dense.bias', 'encoder.layer.23.attention.self.key.weight', 'encoder.layer.17.output.dense.weight', 'encoder.layer.23.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.14.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.16.intermediate.dense.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.23.attention.output.LayerNorm.weight', 'encoder.layer.12.attention.output.dense.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.21.attention.self.key.weight', 'encoder.layer.21.attention.output.dense.bias', 'encoder.layer.22.attention.self.query.bias', 'encoder.layer.15.output.dense.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.14.attention.output.LayerNorm.bias', 'encoder.layer.22.output.LayerNorm.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.12.intermediate.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.14.attention.self.value.weight', 'encoder.layer.18.attention.self.query.bias', 'encoder.layer.15.output.dense.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.17.attention.self.key.weight', 'encoder.layer.20.intermediate.dense.bias', 'encoder.layer.17.attention.output.dense.bias', 'encoder.layer.21.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.16.attention.self.query.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.23.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.15.attention.output.dense.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.12.output.dense.bias', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.18.intermediate.dense.bias', 'encoder.layer.13.attention.output.dense.bias', 'encoder.layer.13.attention.self.key.weight', 'encoder.layer.14.attention.output.LayerNorm.weight', 'encoder.layer.21.attention.self.value.weight', 'encoder.layer.23.attention.self.value.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.21.output.dense.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.21.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.18.attention.output.dense.weight', 'encoder.layer.16.output.dense.bias', 'encoder.layer.12.attention.self.value.weight', 'encoder.layer.23.output.dense.bias', 'encoder.layer.14.attention.self.query.bias', 'encoder.layer.20.output.LayerNorm.bias', 'encoder.layer.23.attention.output.dense.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.20.attention.output.dense.bias', 'encoder.layer.20.attention.self.value.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.20.attention.self.query.weight', 'encoder.layer.15.attention.self.key.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.22.attention.output.dense.weight', 'encoder.layer.12.attention.output.LayerNorm.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.20.attention.self.key.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.12.output.LayerNorm.weight', 'encoder.layer.16.attention.output.dense.bias', 'encoder.layer.22.output.LayerNorm.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.14.output.dense.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.19.attention.self.query.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.16.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.18.attention.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.17.output.LayerNorm.bias', 'encoder.layer.19.output.LayerNorm.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.18.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.18.attention.self.query.weight', 'encoder.layer.19.intermediate.dense.weight', 'encoder.layer.23.attention.self.key.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.weight', 'pooler.dense.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.14.output.LayerNorm.weight', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.13.attention.self.query.bias', 'encoder.layer.16.attention.output.LayerNorm.bias', 'encoder.layer.19.output.LayerNorm.weight', 'encoder.layer.21.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.15.attention.self.key.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.14.attention.output.dense.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.13.attention.self.value.weight', 'encoder.layer.15.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.13.intermediate.dense.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.15.attention.self.query.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.14.attention.self.key.weight', 'encoder.layer.20.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.17.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.13.attention.output.LayerNorm.weight', 'encoder.layer.13.output.LayerNorm.weight', 'encoder.layer.15.intermediate.dense.weight', 'encoder.layer.22.attention.output.LayerNorm.weight', 'encoder.layer.19.attention.self.query.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.22.attention.self.key.bias', 'encoder.layer.21.attention.output.dense.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.14.intermediate.dense.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.13.attention.output.LayerNorm.bias', 'encoder.layer.17.attention.output.LayerNorm.weight', 'encoder.layer.13.output.LayerNorm.bias', 'encoder.layer.14.attention.self.query.weight', 'encoder.layer.16.attention.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.22.intermediate.dense.bias', 'encoder.layer.13.intermediate.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.12.attention.self.query.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.19.output.dense.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.19.attention.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.13.output.dense.bias', 'encoder.layer.19.intermediate.dense.bias', 'encoder.layer.20.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.19.attention.output.dense.bias', 'encoder.layer.18.output.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.21.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.23.attention.self.query.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.18.output.LayerNorm.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.12.attention.self.value.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.15.output.LayerNorm.bias', 'encoder.layer.18.attention.self.value.bias', 'encoder.layer.12.attention.self.query.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.17.attention.self.value.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.16.output.LayerNorm.bias', 'encoder.layer.12.attention.self.key.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.17.output.dense.bias', 'encoder.layer.17.attention.self.key.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.20.attention.self.query.bias', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.21.attention.self.query.weight', 'encoder.layer.18.output.LayerNorm.weight', 'encoder.layer.19.attention.self.value.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.12.attention.output.dense.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.12.output.LayerNorm.bias', 'encoder.layer.15.attention.self.query.bias', 'encoder.layer.18.attention.self.key.bias', 'encoder.layer.21.attention.self.key.bias', 'encoder.layer.22.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.23.output.LayerNorm.weight', 'encoder.layer.23.attention.self.value.weight', 'encoder.layer.14.attention.self.value.bias', 'encoder.layer.16.output.dense.weight', 'encoder.layer.16.attention.self.query.bias', 'encoder.layer.15.attention.output.dense.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.16.attention.self.value.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.20.output.dense.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.1.attention.self.value.weight', 'pooler.dense.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.17.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.22.attention.output.LayerNorm.bias', 'encoder.layer.15.attention.output.LayerNorm.bias', 'encoder.layer.18.attention.self.value.weight', 'encoder.layer.20.attention.self.value.weight', 'encoder.layer.23.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.17.attention.self.query.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.16.attention.self.key.weight', 'encoder.layer.23.output.dense.weight', 'encoder.layer.20.output.LayerNorm.weight', 'encoder.layer.22.output.dense.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.15.intermediate.dense.bias', 'encoder.layer.17.attention.self.query.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.15.output.LayerNorm.weight', 'encoder.layer.16.attention.output.dense.weight', 'encoder.layer.20.intermediate.dense.weight', 'encoder.layer.17.attention.output.dense.weight', 'encoder.layer.17.attention.self.value.bias', 'encoder.layer.19.attention.self.key.bias', 'encoder.layer.20.attention.output.dense.weight', 'encoder.layer.21.attention.output.LayerNorm.bias', 'encoder.layer.12.intermediate.dense.weight', 'encoder.layer.19.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.17.intermediate.dense.bias', 'encoder.layer.19.output.dense.weight', 'encoder.layer.17.intermediate.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.20.attention.self.key.weight', 'encoder.layer.21.attention.self.value.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.21.attention.self.query.bias', 'encoder.layer.15.attention.self.value.bias', 'encoder.layer.18.attention.self.key.weight', 'encoder.layer.21.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.14.attention.self.key.bias', 'encoder.layer.16.attention.self.key.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.18.attention.output.dense.bias', 'encoder.layer.14.output.dense.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.21.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.dense.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.15.attention.self.value.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.22.attention.self.key.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.18.intermediate.dense.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.13.attention.output.dense.weight', 'encoder.layer.16.attention.self.value.bias', 'encoder.layer.13.attention.self.query.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.23.attention.self.query.weight', 'encoder.layer.12.output.dense.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.22.attention.self.value.bias', 'encoder.layer.22.output.dense.bias', 'encoder.layer.13.attention.self.key.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.23.intermediate.dense.weight', 'encoder.layer.18.output.dense.bias', 'encoder.layer.12.attention.self.key.bias', 'encoder.layer.16.intermediate.dense.bias', 'encoder.layer.23.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.10.attention.self.value.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'ElectraTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ClassifyNews(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 1024, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 1024)\n",
       "      (token_type_embeddings): Embedding(2, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (12): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (13): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (14): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (15): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (16): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (17): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (18): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (19): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (20): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (21): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (22): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (23): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (relu): ReLU()\n",
       "  (fc1): Linear(in_features=1024, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model_name = \"csebuetnlp/banglabert_large\"\n",
    "bert = BertModel.from_pretrained(bert_model_name)\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "model = ClassifyNews(bert)\n",
    "\n",
    "saved_model_path = f'saved_weights/1678450980.0084934-news_large.pth'\n",
    "\n",
    "checkpoint = torch.load(saved_model_path)\n",
    "\n",
    "model.load_state_dict(checkpoint)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8eef044e-b7f9-4e83-be41-57055a3363e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(f'data/cleaned_data.csv')\n",
    "\n",
    "_, test_data = train_test_split(data, test_size = 0.2, random_state = 2023, stratify = data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "eb5d77be-428d-4867-b5c3-865916970358",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_data_file = NewsDatasets(test_data)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_data_file, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bbb8bdac-d670-446e-a166-5d5d501141b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "real_data = next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3849cc5b-40f1-4806-87ef-a57aadf89ad6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text, labels = real_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d14872bf-1347-4747-aa4b-b4c984fa48fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "206317e8-6447-45b1-98fd-30fe539927c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a0efeff7-0d4b-400d-96cd-2346c22192ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f1524ce2-7c8f-402e-b92e-2907212f9fc5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'      [SEP]                                                                   '"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e2492bfd-229d-46fd-b9cf-b2a22d8c0103",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['international'], dtype=object)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_enc.inverse_transform(labels.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f88cf7-9d72-4c3d-8c0a-c2b6dcbd214b",
   "metadata": {},
   "source": [
    "## Create input and baseline for explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8803211e-8dbb-4b01-9311-fddbfa17a90e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def input_and_baseline(text):\n",
    "    \n",
    "    tokenizer_config = {\n",
    "        \"max_length\": 100,\n",
    "        \"truncation\": True,\n",
    "        \"add_special_tokens\": False\n",
    "    }\n",
    "    \n",
    "    baseline_token_id = tokenizer.pad_token_id \n",
    "    sep_token_id = tokenizer.sep_token_id \n",
    "    cls_token_id = tokenizer.cls_token_id \n",
    "\n",
    "    text_ids = tokenizer.encode(text, **tokenizer_config)\n",
    "    \n",
    "    input_ids = [cls_token_id] + text_ids + [sep_token_id]\n",
    "   \n",
    "    raw_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "  \n",
    "\n",
    "    baseline_input_ids = [cls_token_id] + [baseline_token_id] * len(text_ids) + [sep_token_id]\n",
    "    \n",
    "    return torch.tensor([input_ids], device = 'cpu'), torch.tensor([baseline_input_ids], device = 'cpu'), raw_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5d365d4c-7edb-4d45-b883-e491f6f9c269",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_ids, baseline_input_ids, all_tokens = input_and_baseline(text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "17c2c4c9-7357-421a-afb5-b246b6761be1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    2,  2415,  7481,   411,  3163, 17957,  2927, 21384,   764,  7363,\n",
       "             3,  6305,  2415,  7481,   411,  3163, 17957,  7363,  6654, 21384,\n",
       "          3763, 13980, 22516,  1613,  7481,   411,     1,  5172,  1460,     1,\n",
       "          4365,  3606,     1,  5678,     1,  4114, 17318,  5082,  2846,     1,\n",
       "          4365,  3606,  6031,  5108,  5335,  1780,     1,  3617,  3517,  5427,\n",
       "          3905,     1,  4085, 10250, 21384,   764,  6507,     1,  2927,  2308,\n",
       "          7377, 18663,   936, 30070,   411,  2708,     1,  1613,  6031, 13764,\n",
       "         20015, 18254, 11222,  3794, 11384,  9832,  2562,  4960,  7995,  3592,\n",
       "          3808,  3854,  1615,  5108,  1833,     3]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4b7b71d0-b014-453f-84ec-635c0a5f660b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c4691fa1-4f4d-4405-b9eb-1876d1ffa26e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]   ##     ##  [SEP]    ##           ## [UNK]   [UNK]   [UNK]  [UNK]     [UNK]       [UNK]     [UNK]    ##  [UNK]   ##   ## ##  [UNK]                   [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join(all_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "bbbf591d-9e94-415c-af81-798166f884b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def model_output(inputs):\n",
    "    return model(inputs)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c36080bd-3f9d-4d43-b996-1dc68e00174b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lig = LayerIntegratedGradients(model_output, model.bert.embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8da7d9c9-c2ab-4b41-9303-bc70b9a81dcf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "attributions, delta = lig.attribute(inputs= input_ids.to(device),\n",
    "                                    baselines= baseline_input_ids.to(device),\n",
    "                                    n_steps = 500,\n",
    "                                    return_convergence_delta=True\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3c56bd10-96b5-4bdb-84f1-04aa7f157111",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 86, 1024])\n"
     ]
    }
   ],
   "source": [
    "print(attributions.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d4138344-fcf0-4715-af78-d3b8beb39b40",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(4)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(model(input_ids)[0]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "cb951916-47f6-4e07-9fe6-bab01eda6494",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.8337, grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(model(input_ids)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2d0b669b-1e28-4a91-a0c0-435ebea4590c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.5820,  0.0872, -0.7721, -0.0417,  4.8337, -0.3294,  1.7847, -0.9160,\n",
       "         -0.2395]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9c79077d-6065-4462-a9df-4fc4e2bacdcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def summarize_attributions(attributions):\n",
    "\n",
    "    attributions = attributions.sum(dim=2).squeeze(0)\n",
    "    attributions = attributions / torch.norm(attributions)\n",
    "    attributions = attributions.cpu().detach().numpy()\n",
    "    \n",
    "    return attributions\n",
    "\n",
    "attributions_sum = summarize_attributions(attributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "262971ba-84c5-4ce9-b888-e08186112824",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>4</b></text></td><td><text style=\"padding-right:2em\"><b>4 (4.83)</b></text></td><td><text style=\"padding-right:2em\"><b>('      [SEP]                                                                   ',)</b></text></td><td><text style=\"padding-right:2em\"><b>1.50</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(0, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 84%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [UNK]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 85%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [UNK]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(0, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [UNK]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [UNK]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [UNK]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [UNK]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [UNK]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [UNK]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##                    </font></mark><mark style=\"background-color: hsl(120, 75%, 83%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [UNK]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>4</b></text></td><td><text style=\"padding-right:2em\"><b>4 (4.83)</b></text></td><td><text style=\"padding-right:2em\"><b>('      [SEP]                                                                   ',)</b></text></td><td><text style=\"padding-right:2em\"><b>1.50</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(0, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 84%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [UNK]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 85%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [UNK]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(0, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [UNK]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [UNK]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [UNK]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [UNK]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [UNK]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [UNK]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##                    </font></mark><mark style=\"background-color: hsl(120, 75%, 83%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [UNK]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">                     </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_vis = viz.VisualizationDataRecord(\n",
    "                        word_attributions = attributions_sum,\n",
    "                        pred_prob = torch.max(model(input_ids)[0]),\n",
    "                        pred_class = torch.argmax(model(input_ids)[0]).numpy(),\n",
    "                        true_class = labels.item(),\n",
    "                        attr_class = text,\n",
    "                        attr_score = attributions_sum.sum(),       \n",
    "                        raw_input_ids = all_tokens,\n",
    "                        convergence_score = delta)\n",
    "\n",
    "viz.visualize_text([score_vis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df021b5d-806a-4ff7-ba78-4ddee78f30ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:explainable-news] *",
   "language": "python",
   "name": "conda-env-explainable-news-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
